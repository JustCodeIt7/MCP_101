import asyncio
import os
import sys
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio

# --- Configuration ---
OLLAMA_MODEL = 'ollama/llama3.2'  # Replace with your desired Ollama model (e.g., ollama/llama3)
# Ensure Ollama server is running and the model is pulled.
MCP_SERVER_SCRIPT_NAME = "mcp_server_app.py"

# --- MCP Server Code (to be written to MCP_SERVER_SCRIPT_NAME) ---
# This is based on your provided server code, with additions for stdio execution,
# explicit naming, improved docstrings, and logging.
mcp_server_code = """
import logging
import sys
# It's assumed the user has the 'mcp' library installed, providing 'FastMCP'.
# This typically comes from a package like 'mcp-server-python'.
from mcp.server.fastmcp import FastMCP

# Configure logging for the MCP server to output to stderr.
# This prevents log messages from interfering with MCP's JSON communication over stdout.
logging.basicConfig(
    level=logging.INFO, # Set to DEBUG for more verbose server logs
    stream=sys.stderr,
    format='[MCP Server - %(levelname)s - %(asctime)s]: %(message)s'
)
logger = logging.getLogger(__name__)

# Optionally, set the mcp library's own logger level.
# logging.getLogger("mcp").setLevel(logging.INFO) # Or logging.WARNING

# Create an MCP server instance named "HelloWorld"
mcp = FastMCP("HelloWorld")
logger.info(f"FastMCP server '{mcp.name}' initialized.")

# 1. Define a Resource
# Resources expose data to LLMs, like GET endpoints in a REST API.
@mcp.resource("hello://world")
def hello_resource() -> str:
    '''Returns the static string "Hello, world!". This resource provides a fixed greeting message.'''
    logger.info(f"Executing resource: hello://world")
    return "Hello, world!"

# 2. Define a Prompt
# Prompts are templates that help LLMs interact with your server effectively.
# Explicitly named "hello_user_prompt" via the decorator.
@mcp.prompt("hello_user_prompt")
def generate_greeting_prompt(name: str) -> str: # Function name changed to be more descriptive
    '''Generates a personalized greeting string using a template. Input: name (str). Output: formatted greeting string.'''
    logger.info(f"Executing prompt 'hello_user_prompt' for name: {name}")
    prompt_template = f"Hello, {name}! This is a response generated by the 'hello_user_prompt'."
    return prompt_template

# 3. Define a Tool
# Tools enable LLMs to execute actions via your server.
# Explicitly named "hello_user_tool" via the decorator.
@mcp.tool("hello_user_tool")
def interactive_hello_tool(name: str) -> str: # Function name changed
    '''Tool that takes a name, processes it (e.g., title cases), and returns a personalized greeting string. Input: name (str). Output: formatted greeting string.'''
    logger.info(f"Executing tool 'hello_user_tool' with name: {name}")
    # Example of a simple computation/side effect
    processed_name = name.strip().title()
    return f"Hello, world! The 'hello_user_tool' greets you, {processed_name}!"

if __name__ == "__main__":
    logger.info(f"Attempting to start MCP server '{mcp.name}' in stdio mode...")
    try:
        # The method to run an MCP server in stdio mode can vary depending on the library version.
        # 'run_stdio()' is a common method name in Python MCP server implementations.
        if hasattr(mcp, 'run_stdio'):
            mcp.run_stdio()
        elif hasattr(mcp, 'serve_stdio'): # Another possible method name
            mcp.serve_stdio()
        else:
            logger.error(
                "The FastMCP instance does not have a 'run_stdio()' or 'serve_stdio()' method. "
                "Please check the documentation for your 'mcp.server.fastmcp' library "
                "on how to start the server in stdio mode for PydanticAI integration."
            )
            sys.exit(1) # Exit if server cannot be started correctly
    except Exception as e:
        logger.error(f"Failed to start MCP server in stdio mode: {e}", exc_info=True)
        sys.exit(1)
"""

async def run_agent_interactions(agent: Agent):
    """
    Runs a series of interactions with the PydanticAI agent
    to demonstrate usage of the MCP server's resource, prompt, and tool.
    """
    print("\n--- Interacting with PydanticAI Agent (using MCP Server) ---")

    # The PydanticAI MCP client makes resources, prompts, and tools from the MCP server
    # available to the LLM. The LLM uses the names and docstrings (descriptions) of these items
    # to decide when and how to use them.
    # The naming convention is often `ServerName.verb_ItemName` (e.g., `HelloWorld.getResource_hello_world`).

    # 1. Use the resource "hello://world"
    print("\n[1] Querying the MCP resource 'hello://world' via the LLM...")
    resource_query = "What message does the 'hello://world' resource on the 'HelloWorld' server provide?"
    try:
        result = await agent.run(resource_query)
        print(f"LLM Response (Resource Query): {result.output}")
        # Expected: LLM identifies the relevant "tool" exposed by PydanticAI for this resource,
        # calls it, receives "Hello, world!", and incorporates this into its answer.
    except Exception as e:
        print(f"Error querying resource: {e}")

    # 2. Use the prompt "hello_user_prompt"
    print("\n[2] Using the MCP prompt 'hello_user_prompt' via the LLM...")
    prompt_query = "Please use the 'hello_user_prompt' from the 'HelloWorld' server to generate a greeting for 'Alice'."
    try:
        result = await agent.run(prompt_query)
        print(f"LLM Response (Prompt Usage): {result.output}")
        # Expected: LLM identifies the tool for 'hello_user_prompt', calls it with name="Alice",
        # receives the templated string, and presents it.
    except Exception as e:
        print(f"Error using prompt: {e}")

    # 3. Use the tool "hello_user_tool"
    print("\n[3] Executing the MCP tool 'hello_user_tool' via the LLM...")
    tool_query = "Can you use the 'hello_user_tool' available on the 'HelloWorld' server to greet 'Bob Smith'?"
    try:
        result = await agent.run(tool_query)
        print(f"LLM Response (Tool Execution): {result.output}")
        # Expected: LLM identifies 'hello_user_tool', calls it with name="Bob Smith",
        # receives the tool's output, and presents it.
    except Exception as e:
        print(f"Error using tool: {e}")


async def main():
    """
    Main function to set up the MCP server, PydanticAI agent, and run interactions.
    """
    print("--- PydanticAI with Custom Python MCP Server (Stdio) Demo ---")

    # Step 1: Write the MCP server Python script to a file
    try:
        with open(MCP_SERVER_SCRIPT_NAME, "w") as f:
            f.write(mcp_server_code)
        print(f"Successfully wrote MCP server script to '{MCP_SERVER_SCRIPT_NAME}'.")
    except IOError as e:
        print(f"Fatal: Error writing MCP server script: {e}")
        return

    # Step 2: Define the MCP server process using MCPServerStdio
    # This will run `python mcp_server_app.py` (or the interpreter specified by sys.executable) as a subprocess.
    print(f"\nConfiguring MCPServerStdio to run: {sys.executable} {MCP_SERVER_SCRIPT_NAME}")
    mcp_server_process = MCPServerStdio(
        sys.executable,  # Use the current Python interpreter
        args=[MCP_SERVER_SCRIPT_NAME],
        # If your server script has relative path dependencies, you might need to set cwd:
        # cwd=os.path.dirname(os.path.abspath(__file__)) # Example: sets CWD to this script's dir
    )

    # Step 3: Initialize the PydanticAI Agent
    # It uses the specified Ollama model and the MCP server defined above.
    print(f"\nInitializing PydanticAI Agent with LLM: {OLLAMA_MODEL}...")
    try:
        agent = Agent(
            llm=OLLAMA_MODEL,
            mcp_servers=[mcp_server_process],
            # For more detailed PydanticAI logging, you can set log_level:
            # from pydantic_ai.common import LogLevel
            # log_level=LogLevel.DEBUG
        )
        print("PydanticAI Agent initialized successfully.")
    except Exception as e:
        print(f"Fatal: Error initializing PydanticAI Agent: {e}")
        print("Please ensure:")
        print("  - Your Ollama server is running (`ollama serve`).")
        print(f"  - The model '{OLLAMA_MODEL}' is pulled (`ollama pull {OLLAMA_MODEL.split('/')[-1]}`).")
        print("  - PydanticAI with MCP extras is installed ('pip install \"pydantic-ai-slim[mcp]\"').")
        print("  - The MCP server library (for FastMCP) is installed.")
        return

    # Step 4: Run the MCP server(s) and interact with the agent
    # The `agent.run_mcp_servers()` context manager handles starting and stopping the MCP server subprocess.
    print("\nStarting MCP server via PydanticAI and preparing for agent interactions...")
    try:
        async with agent.run_mcp_servers():
            print("MCP server process should now be running in the background managed by PydanticAI.")
            await run_agent_interactions(agent)
        print("\nAgent interactions complete. MCP server process has been stopped by PydanticAI.")
    except Exception as e:
        print(f"An error occurred during agent interactions or MCP server management: {e}")
    finally:
        # Clean up the created MCP server script
        if os.path.exists(MCP_SERVER_SCRIPT_NAME):
            try:
                os.remove(MCP_SERVER_SCRIPT_NAME)
                print(f"\nCleaned up: Removed temporary script '{MCP_SERVER_SCRIPT_NAME}'.")
            except OSError as e:
                print(f"Warning: Error removing '{MCP_SERVER_SCRIPT_NAME}': {e}")

if __name__ == "__main__":
    print("This script demonstrates using PydanticAI with a custom Python MCP server via stdio.")
    print("Please ensure all prerequisites are met before running:")
    print("  1. Python 3.10 or higher is installed and used.")
    print("  2. PydanticAI with MCP extras installed: pip install \"pydantic-ai-slim[mcp]\"")
    print("  3. An MCP server library (providing mcp.server.fastmcp, e.g., mcp-server-python) is installed.")
    print("  4. Ollama server is running (command: `ollama serve` in a separate terminal).")
    print(f"  5. The Ollama model '{OLLAMA_MODEL}' is pulled (command: `ollama pull {OLLAMA_MODEL.split('/')[-1]}`).")
    print("------------------------------------------------------------------------------------")

    # Optional: Configure client-side logging for PydanticAI or other libraries
    # logging.basicConfig(level=logging.INFO, format='[PydanticAI Client - %(levelname)s]: %(message)s')

    # If you use Logfire for observability (https://logfire.pydantic.dev/):
    # import logfire
    # logfire.configure()
    # logfire.instrument_pydantic_ai()

    asyncio.run(main())
